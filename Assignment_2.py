# -*- coding: utf-8 -*-
"""NLP_Assignment_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B_Z_MVVTpxL8Jw1c-24L8yZgmSqHEDqK
"""

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()

bow_matrix = vectorizer.fit_transform(corpus)

feature_name = vectorizer.get_feature_names_out()

bow_dense = bow_matrix.toarray()

print("\nBag of Words Matrix:")
print(bow_dense)

print("\nWord to Index Mapping:", vectorizer.vocabulary_)

from sklearn.feature_extraction.text import TfidfTransformer

from sklearn.preprocessing import normalize

from gensim.models import Word2Vec
import numpy as np

# Sample corpus
corpus = [
    "Artificial Intelligence is the future of technology.",
    "Machine learning is a branch of Artificial Intelligence.",
    "Natural Language Processing is an application of Machine Learning."
]

# Step 1: Bag of Words (BoW) - Count Occurrence
vectorizer = CountVectorizer()
bow_matrix = vectorizer.fit_transform(corpus)
feature_names = vectorizer.get_feature_names_out()
print("Feature Names:", feature_names)
print("\nBag of Words Matrix:")
print(bow_matrix.toarray())

# Step 2: Normalize Count Occurrence
normalized_bow = normalize(bow_matrix, norm='l1', axis=1)
print("\nNormalized Bag of Words Matrix:")
print(normalized_bow.toarray())

# Step 3: TF-IDF Transformation
tfidf_transformer = TfidfTransformer()
tfidf_matrix = tfidf_transformer.fit_transform(bow_matrix)
print("\nTF-IDF Matrix:")
print(tfidf_matrix.toarray())

# Step 4: Word2Vec Embeddings
# Preprocess corpus for Word2Vec (tokenization)
tokenized_corpus = [sentence.lower().split() for sentence in corpus]

# Train Word2Vec model
word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)

# Get word embeddings
print("\nWord2Vec Embeddings:")
for word in word2vec_model.wv.index_to_key:
    print(f"{word}: {word2vec_model.wv[word]}")