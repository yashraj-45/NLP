# -*- coding: utf-8 -*-
"""Assignment 1_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uNtamE3V2xbqtKu5Hw5BCgWg5kLhE7hp
"""

import nltk
from nltk.tokenize import word_tokenize, WordPunctTokenizer, TreebankWordTokenizer, TweetTokenizer
from nltk.stem import PorterStemmer, SnowballStemmer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Sample text
text = "Just saw the most amazing sunset! ðŸŒ… #beautiful #nature #sunsetlovers @viraj"

# Tokenization (Whitespace, Punctuation-based, Treebank, Tweet)
whitespace_tokenized = text.split()
print("Whitespace Tokenization:", whitespace_tokenized)

wordpunct_tokenizer = WordPunctTokenizer()
wordpunct_tokenized = wordpunct_tokenizer.tokenize(text)
print("Punctuation-based Tokenization:", wordpunct_tokenized)

treebank_tokenizer = TreebankWordTokenizer()
treebank_tokenized = treebank_tokenizer.tokenize(text)
print("Treebank Tokenization:", treebank_tokenized)

tweet_tokenizer = TweetTokenizer()
tweet_tokenized = tweet_tokenizer.tokenize(text)
print("Tweet Tokenization:", tweet_tokenized)

mwe_tokenized = text.split()
print("MWE Tokenization:", mwe_tokenized)

porter_stemmer = PorterStemmer()
snowball_stemmer = SnowballStemmer('english')

porter_stemmed = [porter_stemmer.stem(word) for word in wordpunct_tokenized]
print("Porter Stemming:", porter_stemmed)

snowball_stemmed = [snowball_stemmer.stem(word) for word in wordpunct_tokenized]
print("Snowball Stemming:", snowball_stemmed)

lemmatizer = WordNetLemmatizer()

lemmatized = [lemmatizer.lemmatize(word) for word in wordpunct_tokenized]
print("Lemmatization:", lemmatized)